{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from notebookutils import mssparkutils\n",
        "from pyspark.sql.functions import lit\n",
        "from functools import reduce\n",
        "from datetime import datetime\n",
        "\n",
        "# Step 1: Set paths\n",
        "input_path = \"abfss://raw-data@datapipelineprojectsa123.dfs.core.windows.net/\"\n",
        "output_base = \"abfss://clean-data@datapipelineprojectsa123.dfs.core.windows.net/\"\n",
        "\n",
        "# Step 2: List all CSV files using mssparkutils\n",
        "files = mssparkutils.fs.ls(input_path)\n",
        "csv_files = [f.path for f in files if f.path.endswith(\".csv\")]\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Define expected columns\n",
        "expected_columns = [\"OrderID\", \"OrderDate\", \"CustomerName\", \"Region\", \"Product\", \"Category\", \"Quantity\", \"UnitPrice\", \"Amount\"]\n",
        "dfs = []\n",
        "\n",
        "# Step 4: Load, align, and clean each file\n",
        "for path in csv_files:\n",
        "    try:\n",
        "        df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(path)\n",
        "\n",
        "        # Skip empty files\n",
        "        if len(df.columns) == 0:\n",
        "            print(f\"⚠️ Skipping empty file: {path}\")\n",
        "            continue\n",
        "\n",
        "        # Add missing columns as nulls\n",
        "        for col in expected_columns:\n",
        "            if col not in df.columns:\n",
        "                df = df.withColumn(col, lit(None))\n",
        "\n",
        "        # Reorder/select columns\n",
        "        df = df.select(*expected_columns)\n",
        "        dfs.append(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to read {path}: {str(e)}\")\n",
        "\n",
        "# Step 5: Combine and write\n",
        "if dfs:\n",
        "    final_df = reduce(lambda a, b: a.unionByName(b), dfs)\n",
        "    final_df_clean = final_df.dropna().dropDuplicates()\n",
        "\n",
        "    folder_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
        "    output_path = f\"{output_base}{folder_name}/\"\n",
        "    final_df_clean.write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
        "    print(f\"✅ Write complete: {output_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ No valid files found to process.\")\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}